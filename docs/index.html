<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>On Calibration of Large Language Models: From Response To Capability</title>
  <style>
@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css');

html {
  -webkit-print-color-adjust: exact;
  print-color-adjust: exact;
}

* {
  box-sizing: border-box;
}

html, body {
  margin: 0;
  padding: 0;
}

a, a.visited {
  color: inherit;
}

h1, h2, h3 {
  letter-spacing: -0.01em;
  line-height: 1.2;
  font-weight: 600;
  margin-bottom: 0;
}

body {
  line-height: 1.5;
  font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display",
    "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji",
    "Segoe UI Symbol";
}

@media only screen {
  body {
    margin: 2em auto;
    max-width: 900px;
    color: rgb(55, 53, 47);
  }
}

a, a.visited {
  text-decoration: underline;
}

.page-title {
  font-size: 2.5rem;
  font-weight: 700;
  margin-top: 0;
  margin-bottom: 0.75em;
}

h1 { font-size: 1.875rem; margin-top: 1.875rem; }
h2 { font-size: 1.5rem; margin-top: 1.5rem; }
h3 { font-size: 1.25rem; margin-top: 1.25rem; }

figure {
  margin: 1.25em 0;
  page-break-inside: avoid;
}

figcaption {
  opacity: 0.5;
  font-size: 85%;
  margin-top: 0.5em;
}

img {
  max-width: 100%;
}

p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

.image {
  border: none;
  margin: 1.5em 0;
  padding: 0;
  border-radius: 0;
  text-align: center;
}

.column-list {
  display: flex;
  justify-content: space-between;
}

.column {
  padding: 0 1em;
}

.column:first-child { padding-left: 0; }
.column:last-child { padding-right: 0; }

.highlight-blue {
  color: rgba(51, 126, 169, 1);
  fill: rgba(51, 126, 169, 1);
}

.highlight-teal {
  color: rgba(68, 131, 97, 1);
  fill: rgba(68, 131, 97, 1);
}

.code, code {
  background: rgba(135, 131, 120, 0.15);
  border-radius: 3px;
  padding: 0.2em 0.4em;
  font-size: 100%;
  tab-size: 2;
}

.code {
  padding: 1.5em 1em;
  font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace;
}

.code-wrap {
  white-space: pre-wrap;
  word-break: break-all;
}

.code > code {
  background: none;
  padding: 0;
  font-size: 100%;
  color: inherit;
}

hr {
  background: transparent;
  display: block;
  width: 100%;
  height: 1px;
  visibility: visible;
  border: none;
  border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

/* Table styles */
table, th, td {
  border: 1px solid rgba(55, 53, 47, 0.09);
  border-collapse: collapse;
}

table {
  border-left: none;
  border-right: none;
  width: 100%;
  margin: 1em 0;
  font-size: 0.875rem;
}

th, td {
  font-weight: normal;
  padding: 0.25em 0.5em;
  line-height: 1.5;
  min-height: 1.5em;
  text-align: left;
}

th {
  color: rgba(55, 53, 47, 0.6);
  font-weight: 500;
  background: rgb(247, 246, 243);
}

/* Navigation */
.nav-link-wrap {
  text-align: center;
  margin-top: 2.5em;
}

.button-link {
  text-decoration: none;
  display: inline-flex;
  align-items: center;
  gap: 0.4em;
  padding: 1rem 1.2rem;
  border: 1px solid #e5e7eb;
  border-radius: 12px;
  font-size: 1.05rem;
  background: #f9fafb;
  color: inherit;
  white-space: normal;
  transition: background 150ms ease, border-color 150ms ease;
}

.button-link::after {
  content: "\2192";
  font-size: 1.1em;
}

.button-link:hover {
  background: #ffffff;
  border-color: #cbd5f5;
}

.nav-links {
  display: inline-flex;
  align-items: center;
  gap: 0.5rem;
  flex-wrap: wrap;
  justify-content: center;
}

/* Mobile responsive */
@media (max-width: 768px) {
  body {
    margin: 1em;
    padding: 0 0.5em;
  }

  .page-title {
    font-size: 1.75rem;
  }

  h1 { font-size: 1.5rem; }
  h2 { font-size: 1.25rem; }
  h3 { font-size: 1.1rem; }

  .column-list {
    flex-direction: column;
  }

  .column {
    width: 100% !important;
    padding: 0;
    margin-bottom: 1em;
  }

  .column:first-child { margin-bottom: 0; }

  table {
    font-size: 0.75rem;
  }

  pre.code {
    font-size: 0.75rem;
    padding: 1em 0.75em;
    overflow-x: auto;
  }
}
  </style>
</head>
<body>
  <article class="page sans">
    <header>
      <h1 class="page-title">On Calibration of Large Language Models:</br>From Response To Capability</h1>
    </header>

    <div class="page-body">

      <!-- Authors -->
      <p>
        <a href="https://www.linkedin.com/in/sinhanyang/" style="text-decoration:underline">Sin-Han Yang</a><sup>1*</sup>, &nbsp;
        <a href="https://brian-ckwu.github.io/" style="text-decoration:underline">Cheng-Kuang Wu</a><sup>1*</sup>, &nbsp;
        Chieh-Yen Lin<sup>1</sup>, &nbsp;
        <a href="https://www.csie.ntu.edu.tw/~yvchen/" style="text-decoration:underline">Yun-Nung Chen</a><sup>2</sup>, &nbsp;
        <a href="https://speech.ee.ntu.edu.tw/~hylee/index.php" style="text-decoration:underline">Hung-yi Lee</a><sup>2</sup>, &nbsp;
        <a href="https://shaohua0116.github.io/" style="text-decoration:underline">Shao-Hua Sun</a><sup>1,2</sup>
      </p>
      <p><sup>1</sup>Appier AI Research &nbsp;&nbsp; <sup>2</sup>National Taiwan University</p>
      <p>*Equal contribution, junior author listed earlier</p>
      <p>
        <strong>üìù Paper: </strong><a href="https://drive.google.com/file/d/1my8A3HYPHKAVMmBuCWEChYm1ASob19lc/view?usp=sharing" style="text-decoration:underline">Google Drive link</a>, arXiv version coming soon
      </p>
      <p></p>
        <strong>üíª Code: </strong><a href="https://github.com/appier-research/llm-calibration" style="text-decoration:underline">https://github.com/appier-research/llm-calibration</a>
      </p>

      <!-- Figure 1 -->
      <figure class="image" style="text-align:center; margin-bottom:3em;">
        <a href="images/figure1_calibration_definitions.png">
          <img style="width:100%; max-width:720px;" src="images/figure1_calibration_definitions.png"/>
        </a>
        <figcaption>
          <strong>Figure 1:</strong> Definitions of (a) response calibration and our proposed (b) capability calibration.
          Response calibration calibrates the confidence against the correctness of a single output &#x177;.
          Capability calibration calibrates the confidence against the expected accuracy of the model's output distribution &mu;.
        </figcaption>
      </figure>

      <!-- Abstract -->
      <h3><strong>Abstract</strong></h3>
      <p>
        Large language models (LLMs) are widely deployed as general-purpose problem solvers,
        making accurate confidence estimation critical for reliable use. Prior work on LLM calibration
        largely focuses on response-level confidence, which estimates the correctness of a single
        generated output. However, this formulation is misaligned with many practical settings where
        the central question is <em>how likely a model is to solve a query overall</em>.
      </p>
      <p>
        We show that this mismatch results from the stochastic nature of modern LLM decoding,
        under which single-response correctness fails to reflect underlying model capability.
        To address this issue,
        <strong>we introduce capability calibration, which targets the model's expected accuracy on a query.</strong>
        We formally distinguish capability calibration from response calibration and show that the two
        differ both theoretically and empirically. We establish an empirical evaluation setup and study
        a range of confidence estimation methods. Our results demonstrate that capability-calibrated
        confidence improves pass@k prediction and inference budget allocation, establishing a
        foundation with potential for diverse applications.
      </p>

      <!-- Response vs Capability Calibration -->
      <h3><strong>Response Calibration &ne; Capability Calibration</strong></h3>
      <p>
        Calibration ensures that an AI model knows what it doesn't know. 
        It aligns the model's predicted confidence scores with its actual accuracy, 
        meaning an "80% confident" answer should be right 80% of the time. 
        This is critical for reliability, as it allows users to trust high-confidence predictions while 
        easily identifying uncertain ones that require further investigation.
      </p>
      <p>
        Previously, LLM calibration focused on response calibration, which estimates the correctness of a single
        generated output. However, LLMs typically use stochastic decoding (e.g., non-zero temperature sampling), which can
        produce different responses given the same query across inference calls. As a result, the
        correctness of any single sampled response cannot accurately reflect the LLM's underlying
        capability on that query.
      </p>

      <p>
        To address this, we propose <strong>Capability Calibration</strong>. 
        Instead of asking <em>"Was this specific answer correct?"</em>, 
        we ask <em>"What is the probability that the model can answer this correctly?".</em> 
        By targeting the expected accuracy of the model's output distribution, we filter out 
        the noise of generation and capture the model's true reliability.
      </p>

      <!-- Table 1 -->
      <table>
        <thead>
          <tr>
            <th>Definition</th>
            <th>Calibration Target</th>
            <th>Interpretation</th>
            <th>Dependence on LM f<sub>&theta;</sub></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Response Calibration</td>
            <td>Accuracy of &#x177; given x</td>
            <td>How likely is &#x177; correct?</td>
            <td>
              <strong>No.</strong> Since &#x177; is already decoded, the estimation of s(x, &#x177;)
              is decoupled from the generating model f<sub>&theta;</sub>.
            </td>
          </tr>
          <tr>
            <td>Capability Calibration</td>
            <td>Expected accuracy of f<sub>&theta;</sub> given x</td>
            <td>How confident is f<sub>&theta;</sub> in answering x?</td>
            <td>
              <strong>Yes.</strong> The expected accuracy of x is directly
              dependent on f<sub>&theta;</sub>'s capability.
            </td>
          </tr>
        </tbody>
      </table>
      <figcaption>
        <strong>Table 1:</strong> Comparison of calibration definitions. Unlike existing response calibration that
        assesses if the confidence estimate aligns with the correctness of one decoded answer &#x177;,
        our proposed capability calibration evaluates if it aligns with the model's capability
        to answer query x.
      </figcaption>

      <div style="background: #f4f6f8; border-left: 5px solid #2d6da5; padding: 1.25em; margin: 2em 0; border-radius: 4px;">
        <h4 style="margin-top: 0; color: #2d6da5;">Theorem: Decomposing the Loss</h4>
        <p>
          Why do these calibration definitions diverge? We prove that the loss in response calibration 
          is composed of two parts: the true capability loss plus the variance of the output.
        </p>
        
        <div style="text-align: center; font-family: 'Times New Roman', serif; font-size: 1.3em; margin: 1em 0; color: #333;">
          <i style="font-family: serif;">L</i><sub>capability</sub>
          
          &nbsp;=&nbsp; 
          
          &#x1D53C;[<i style="font-family: serif;">L</i><sub>response</sub>]
          
          &nbsp;&minus;&nbsp; 
          
          <span style="color:#d9534f">Var(Output Correctness)</span>
        </div>

        <p style="margin-bottom: 0;">
          <strong>Interpretation:</strong> Response calibration implicitly penalizes the model for the 
          <span style="color:#d9534f">stochasticity (variance)</span> of its generated outputs. 
          By subtracting this variance, Capability Calibration isolates the model's true
          probability of correctness.
        </p>
      </div>

      <div class="column-list" style="align-items: center; margin-top: 2em; margin-bottom: 2em;">
        <div style="width:45%; padding-right: 1.5em;" class="column">
          <h4 style="margin-top:0;"><strong>Visualizing the Divergence</strong></h4>
          <p>
            We empirically show the divergence between two calibration definitions in <strong>Figure 2</strong>. We plot the binary 
            <em>Response Calibration</em> target (y-axis) against the continuous 
            <em>Capability Calibration</em> target (x-axis).
          </p>
          <p>
            The data reveals a significant "spread." Instances where a single response was correct 
            (y=1) often span the full range of actual capability (x &isin; [0, 1]). This confirms that 
            <strong>single-response outcomes are noisy proxies</strong> for the model's true ability.
          </p>
        </div>
        <div style="width:55%;" class="column">
          <figure class="image" style="margin: 0;">
            <a href="images/figure2_divergence.png">
              <img style="width:100%; border: 1px solid #eee; border-radius: 8px;" src="images/figure2_divergence.png" alt="Figure 2: Divergence of Targets"/>
            </a>
            <figcaption style="text-align: center;">
              <strong>Figure 2:</strong> Divergence of calibration targets. RC targets (y-axis) are binary, while CC targets (x-axis) are continuous.
            </figcaption>
          </figure>
        </div>
      </div>

      <!-- Probing: Cost-Performance Tradeoff -->
      <h3><strong>Confidence Estimation: Probing Has the Best Cost-Performance Tradeoff</strong></h3>
      <div class="column-list">
        <div style="width:43.75%" class="column">
          <p>
            After defining the capability calibration target, we seek good capability-calibrated confidence estimators.
            A practically useful confidence estimation method should have both acceptable inference
            cost and good calibration performance. We evaluate common confidence estimators and uncertainty quantification methods alongside our adapted linear probes. 
            This evaluation compares the inference cost against the calibration
            performance (1 - Brier score), where the upper-left corner is the ideal region.
          </p>
          <p>
            <strong>Among all evaluated methods, training linear probes on
            LLM activations is the only one that consistently falls in this ideal region</strong>
            &mdash; achieving strong calibration with cost less than decoding a single token.
            In contrast, response consistency requires multiple forward passes per query, making it
            impractical for real-time applications.
          </p>
        </div>
        <div style="width:56.25%" class="column">
          <figure class="image" style="margin-top:0;">
            <a href="images/figure3_cost_performance_tradeoff_example.png">
              <img style="width:100%;" src="images/figure3_cost_performance_tradeoff_example.png"/>
            </a>
            <figcaption>
              <strong>Figure 3:</strong> Cost-performance tradeoff of different methods. We compare inference cost
              (x-axis, log-scale) against calibration performance (y-axis, 1 - Brier score).
              Among evaluated methods, probing is the only one that consistently falls in the
              ideal upper-left region.
            </figcaption>
          </figure>
        </div>
      </div>

      <!-- Figure 4: Full Results -->
      <h4><strong>Full Results Across Models and Datasets</strong></h4>
      <figure class="image" style="text-align:center;">
        <a href="images/figure4_cost_performance_tradeoff_all.png">
          <img style="width:100%;" src="images/figure4_cost_performance_tradeoff_all.png"/>
        </a>
        <figcaption>
          <strong>Figure 4:</strong> Cost-performance tradeoff of different confidence estimation methods with three
          LLMs on seven datasets. Probing consistently outperforms the random baseline while
          incurring the lowest cost, while response consistency incurs a cost higher than decoding responses.
        </figcaption>
      </figure>

      <!-- Applications -->
      <h3><strong>Applications</strong></h3>
      <p>We demonstrate that capability calibration enables practical applications:</p>
      <p>
        <strong>Pass@k Prediction:</strong> Capability-calibrated confidence can simulate the pass@k
        performance of each instance <em>without</em> sampling multiple outputs or assuming a prior
        distribution over the dataset. Perfectly capability-calibrated confidence achieves near-perfect
        simulation, and our trained probe outperforms oracle response-calibrated confidence.
      </p>
      <p>
        <strong>Inference Budget Allocation:</strong> Capability-calibrated confidence guides the
        allocation of computational resources across queries, allocating more compute to harder questions.
        Both verbalized confidence and probing outperform uniform
        allocation, and the benefits extend to API-based LLMs.
      </p>

      <div style="display: flex; justify-content: space-between; gap: 20px; margin: 20px 0;">
  
        <div style="flex: 1; text-align: center;">
          <img src="images/table2_passk_simulation.png" alt="Pass@k Comparison Table" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
          <figcaption>
            <strong>Table 2:</strong> Pass@k simulation error (MSE) on the MATH-500 dataset. Perfectly capability-calibrated confidence (Oracle CC) simulates well, whereas the error of perfectly response-calibrated confidence
            (Oracle RC) increases as k scales. Our estimator (Probe-MATH) outperforms the Oracle RC across all
            models by approximating the model's expected accuracy.
          </figcaption>
        </div>
      
        <div style="flex: 1; text-align: center;">
          <img src="images/figure5_budget_alloc.png" alt="Budget Allocations Figure" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
          <figcaption>
            <strong>Figure 5:</strong> Inference budget allocation performance of capability-calibrated confidence. 
            Given N questions, we evaluate the performance (success rate) of different methods under the fixed inference budget <i>N√óB</i>. 
            The Oracle capability-calibrated confidence achieves the best performance. 
            Meanwhile, confidence estimators (verbalized and Probe-MATH) both outperform the Uniform allocation in various budgets.
          </p>
        </div>
      
      </div>

      <p>
        Beyond these two applications, we also discuss other applications in the paper, 
        demonstrating the versatility of capability-calibrated confidence. 
        From enabling safer <strong>selective answering</strong> strategies to supporting efficient infrastructure
        via <strong>model routing</strong>, capability calibration serves as a foundational framework for deploying more reliable and trustworthy LLM systems.
      </p>

      <!-- BibTeX -->
      <h3><strong>BibTeX</strong></h3>
      <pre class="code code-wrap"><code style="white-space:pre-wrap;word-break:break-all">@article{yang2026calibration,
  title={On Calibration of Large Language Models: From Response To Capability},
  author={Sin-Han Yang and Cheng-Kuang Wu and Chieh-Yen Lin and Yun-Nung Chen and Hung-yi Lee and Shao-Hua Sun},
  year={2026}
}</code></pre>

    </div>
  </article>

  <!-- Footer Navigation -->
  <div class="page-body">
    <p class="nav-link-wrap">
      <span class="nav-links">
        <a class="button-link" href="https://github.com/appier-research/llm-calibration">
          <span class="button-label">View Code on GitHub</span>
        </a>
      </span>
    </p>
  </div>
</body>
</html>
